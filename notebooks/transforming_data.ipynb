{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7972f9a-5f0e-4874-b598-cead3fb762c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw data file: 202412.csv\n",
      "Transforming data...\n",
      "Uploading transformed data file: 202412_transformed.csv\n",
      "Transformation and upload completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "connection_string = \"ADL CONNECTION STRING\"\n",
    "container_name = \"gold-forecasting-container\"\n",
    "raw_folder = \"raw-data\"\n",
    "transformed_folder = \"transformed-data\"\n",
    "\n",
    "def get_current_month_file():\n",
    "    now = datetime.now()\n",
    "    return f\"{now.year}{now.month:02d}.csv\"\n",
    "\n",
    "def get_transformed_file_name():\n",
    "    now = datetime.now()\n",
    "    return f\"{now.year}{now.month:02d}_transformed.csv\"\n",
    "\n",
    "def download_blob_to_dataframe(blob_service_client, container_name, blob_name):\n",
    "    \"\"\"Download a blob and load it into a Pandas DataFrame.\"\"\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    if blob_client.exists():\n",
    "        blob_data = blob_client.download_blob().readall()\n",
    "        return pd.read_csv(StringIO(blob_data.decode('utf-8')))\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The file {blob_name} does not exist in the container {container_name}.\")\n",
    "\n",
    "def save_dataframe_to_blob(blob_service_client, container_name, blob_name, dataframe):\n",
    "    \"\"\"Save a Pandas DataFrame to Azure Blob Storage as a CSV file.\"\"\"\n",
    "    csv_buffer = StringIO()\n",
    "    dataframe.to_csv(csv_buffer, index=False)\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    blob_client.upload_blob(csv_buffer.getvalue(), overwrite=True)\n",
    "\n",
    "def transform_data(dataframe):\n",
    "    \"\"\"Transforming the raw dataset: ensure no missing dates, interpolate prices, and remove duplicates.\"\"\"\n",
    "    transformed_df = dataframe[[\"date\", \"price\"]].copy()\n",
    "\n",
    "    transformed_df[\"price\"] = pd.to_numeric(transformed_df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "    transformed_df[\"date\"] = pd.to_datetime(transformed_df[\"date\"], errors=\"coerce\").dt.date\n",
    "\n",
    "    if transformed_df[\"date\"].isnull().any():\n",
    "        start_date = transformed_df[\"date\"].min() or datetime(datetime.now().year, datetime.now().month, 1).date()\n",
    "        end_date = transformed_df[\"date\"].max() or (start_date + timedelta(days=30))\n",
    "        complete_dates = pd.date_range(start=start_date, end=end_date, freq=\"D\").date\n",
    "        transformed_df = transformed_df.set_index(\"date\").reindex(complete_dates).reset_index()\n",
    "        transformed_df.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "\n",
    "    transformed_df[\"price\"] = transformed_df[\"price\"].interpolate(method=\"linear\")\n",
    "\n",
    "    transformed_df = transformed_df.drop_duplicates()\n",
    "\n",
    "    return transformed_df\n",
    "\n",
    "def main():\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    raw_file_name = get_current_month_file()\n",
    "    transformed_file_name = get_transformed_file_name()\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading raw data file: {raw_file_name}\")\n",
    "        raw_data_df = download_blob_to_dataframe(blob_service_client, container_name, f\"{raw_folder}/{raw_file_name}\")\n",
    "\n",
    "        print(\"Transforming data...\")\n",
    "        transformed_data_df = transform_data(raw_data_df)\n",
    "\n",
    "        print(f\"Uploading transformed data file: {transformed_file_name}\")\n",
    "        save_dataframe_to_blob(blob_service_client, container_name, f\"{transformed_folder}/{transformed_file_name}\", transformed_data_df)\n",
    "        print(\"Transformation and upload completed successfully.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ba600b-8c34-4cf9-afd7-bf34ee5375c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
