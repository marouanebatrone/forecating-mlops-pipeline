{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7b91a5-de37-451f-ab41-f9d7a93cc4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: azure-storage-blob in /databricks/python3/lib/python3.11/site-packages (12.19.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (1.30.2)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (41.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (4.10.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.7.22)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#%python\n",
    "#%pip install requests\n",
    "#%pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac99f0c-f7be-43a2-86ef-18352b99c9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from io import StringIO\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import io\n",
    "\n",
    "# Function to get yesterday's date\n",
    "def get_yesterday_date():\n",
    "    yesterday = datetime.now() - timedelta(1)\n",
    "    return yesterday.strftime(\"%Y%m%d\")\n",
    "\n",
    "# API request function to fetch gold price data\n",
    "def make_gapi_request():\n",
    "    api_key = \"goldapi-KEY-io\" \n",
    "    symbol = \"XAU\"\n",
    "    curr = \"USD\"\n",
    "    date = get_yesterday_date()\n",
    "\n",
    "    url = f\"https://www.goldapi.io/api/{symbol}/{curr}/{date}\"\n",
    "\n",
    "    headers = {\n",
    "        \"x-access-token\": api_key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Function to upload the data to Azure Data Lake (Blob Storage)\n",
    "def upload_to_datalake(data, container_name, folder_name, file_name):\n",
    "    connection_string = \"CONNECTION STRING TO ADL\"\n",
    "\n",
    "    # Initialize BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # Convert data to CSV format (using StringIO to simulate a file object)\n",
    "    csv_output = StringIO()\n",
    "    csv_writer = pd.DataFrame(data)\n",
    "\n",
    "    # Write data to CSV\n",
    "    csv_writer.to_csv(csv_output, index=False)\n",
    "    \n",
    "    # Upload the data to Azure Data Lake\n",
    "    blob_client = container_client.get_blob_client(f\"{folder_name}/{file_name}\")\n",
    "    blob_client.upload_blob(csv_output.getvalue(), overwrite=True)\n",
    "\n",
    "    print(f\"Data uploaded to {container_name}/{folder_name}/{file_name}\")\n",
    "\n",
    "# Function to check if the file already exists in the container\n",
    "def check_if_file_exists(container_name, folder_name, file_name):\n",
    "    connection_string = \"CONNECTION STRING TO ADL\"\n",
    "    \n",
    "    # Initialize BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    try:\n",
    "        blob_client = container_client.get_blob_client(f\"{folder_name}/{file_name}\")\n",
    "        blob_client.download_blob()\n",
    "        return True  # File exists\n",
    "    except:\n",
    "        return False  # File does not exist\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Get the data from the API\n",
    "    data = make_gapi_request()\n",
    "\n",
    "    if data and len(data) > 0:  # Check if data is not empty\n",
    "        print(\"API Response:\", data)  # Debug: print the API response to inspect the structure\n",
    "\n",
    "        # Ensure that data is a list of records (wrap in list if it's a single dictionary)\n",
    "        if isinstance(data, dict):\n",
    "            print(\"Data is a single record, wrapping it in a list.\")  # Debug: print a message\n",
    "            data = [data]\n",
    "\n",
    "        # Check the structure of the data after wrapping\n",
    "        print(\"Data after wrapping:\", data)\n",
    "\n",
    "        # Get the current month and year for file naming\n",
    "        current_year_month = datetime.now().strftime(\"%Y%m\")  # e.g., \"202412\"\n",
    "        \n",
    "        # Define file name based on the current month and year\n",
    "        file_name = f\"{current_year_month}.csv\"\n",
    "        \n",
    "        container_name = \"gold-forecasting-container\"  # Azure Data Lake container name\n",
    "        folder_name = \"raw-data\"  # Folder in Data Lake\n",
    "        \n",
    "        # Check if the file already exists in the container\n",
    "        file_exists = check_if_file_exists(container_name, folder_name, file_name)\n",
    "\n",
    "        if file_exists:\n",
    "            # If file exists, append the new data to it\n",
    "            print(f\"File {file_name} exists, appending new data.\")\n",
    "            # Read existing data from the file in Azure Data Lake (via pandas)\n",
    "            blob_client = BlobServiceClient.from_connection_string(\"CONNECTION STRING TO ADL\").get_container_client(container_name).get_blob_client(f\"{folder_name}/{file_name}\")\n",
    "            existing_data = pd.read_csv(io.StringIO(blob_client.download_blob().readall().decode()))\n",
    "            \n",
    "            # Convert new data to DataFrame and append\n",
    "            new_data = pd.DataFrame(data)\n",
    "            updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "            updated_data.to_csv(file_name, index=False)\n",
    "        else:\n",
    "            # If file doesn't exist, create a new one\n",
    "            print(f\"File {file_name} doesn't exist, creating new file.\")\n",
    "            # Convert the new data to a DataFrame and upload as a new file\n",
    "            new_data = pd.DataFrame(data)\n",
    "            new_data.to_csv(file_name, index=False)\n",
    "\n",
    "        # Upload the new or updated file\n",
    "        upload_to_datalake(updated_data if file_exists else new_data, container_name, folder_name, file_name)\n",
    "    else:\n",
    "        print(\"No data retrieved from the API.\")\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83cc3a85-d368-48c7-b71f-b5fdbed18a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fetch data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
