{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b244e04-8263-4615-a9dc-c7c3817c7318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install azure-storage-blob\n",
    "!pip install azureml-sdk\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b5b760-5818-4806-9934-7ec012a67ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a6ec3f-cd07-4567-9b61-4a3e7fdf61c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: transformed-data/202412_transformed.csv\n",
      "Downloaded: latest_gold_history/latest_price_gold.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/databricks/python/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/databricks/python/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/databricks/python/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "connection_string = \"ADL CONNECTION STRING\"\n",
    "container_name = \"gold-forecasting-container\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Function to download the files from Azure Data Lake\n",
    "def download_file_from_blob(container_name, blob_name, local_path):\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    try:\n",
    "        with open(local_path, \"wb\") as file:\n",
    "            file.write(blob_client.download_blob().readall())\n",
    "        print(f\"Downloaded: {blob_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {blob_name}: {str(e)}\")\n",
    "\n",
    "# Get the current year and month\n",
    "current_date = datetime.now()\n",
    "current_year_month = current_date.strftime('%y%m')\n",
    "\n",
    "# Try to list the blobs in the 'transformed-data' container to find the correct file name\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "blobs = container_client.list_blobs(name_starts_with=\"transformed-data/\")\n",
    "\n",
    "# Find the correct file name by checking if it matches the year-month format\n",
    "correct_file_name = None\n",
    "for blob in blobs:\n",
    "    if f\"{current_year_month}_transformed.csv\" in blob.name:\n",
    "        correct_file_name = blob.name\n",
    "        break\n",
    "\n",
    "# If no file is found for the current month, fall back to the most recent file\n",
    "if not correct_file_name:\n",
    "    blobs = container_client.list_blobs(name_starts_with=\"transformed-data/\")\n",
    "    for blob in blobs:\n",
    "        correct_file_name = blob.name\n",
    "        break\n",
    "\n",
    "if correct_file_name:\n",
    "    download_file_from_blob(container_name, correct_file_name, f\"{current_year_month}_transformed.csv\")\n",
    "else:\n",
    "    print(\"No matching transformed data file found.\")\n",
    "\n",
    "# Download the latest price gold CSV file\n",
    "download_file_from_blob(container_name, \"latest_gold_history/latest_price_gold.csv\", \"latest_price_gold.csv\")\n",
    "\n",
    "# Loading the CSV files\n",
    "recent_data = pd.read_csv(f\"{current_year_month}_transformed.csv\")\n",
    "latest_gold_data = pd.read_csv(\"latest_price_gold.csv\")\n",
    "\n",
    "# Merge the two datasets of gold prices\n",
    "merged_data = pd.concat([latest_gold_data, recent_data], ignore_index=True)\n",
    "\n",
    "# Convert 'date' column to datetime, removing the strict format\n",
    "merged_data['date'] = pd.to_datetime(merged_data['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates if any\n",
    "merged_data = merged_data.dropna(subset=['date'])\n",
    "\n",
    "# Remove duplicates based on the 'date' column\n",
    "merged_data = merged_data.drop_duplicates(subset=['date'])\n",
    "\n",
    "# Sort data by 'date'\n",
    "merged_data.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Set 'date' as the index\n",
    "merged_data.set_index('date', inplace=True)\n",
    "\n",
    "# Generate a continuous date range\n",
    "date_range = pd.date_range(start=merged_data.index.min(), end=merged_data.index.max(), freq='D')\n",
    "\n",
    "# Reindex the data with the continuous date range\n",
    "merged_data = merged_data.reindex(date_range)\n",
    "\n",
    "# Interpolate missing data linearly\n",
    "merged_data['price'] = merged_data['price'].interpolate(method='linear')\n",
    "\n",
    "# Reset index and rename\n",
    "merged_data.reset_index(inplace=True)\n",
    "merged_data.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "# Save the merged and cleaned data to a CSV\n",
    "merged_data.to_csv(\"updated_latest_price_gold.csv\", index=False)\n",
    "\n",
    "# Upload the merged file back to Azure Data Lake\n",
    "def upload_file_to_blob(local_path, container_name, blob_name):\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    with open(local_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "# Upload the updated latest gold price CSV\n",
    "upload_file_to_blob(\"updated_latest_price_gold.csv\", container_name, \"latest_gold_history/latest_price_gold.csv\")\n",
    "\n",
    "# Retrain the model with the merged data\n",
    "train_data = merged_data['price'] \n",
    "\n",
    "train_diff = train_data.diff().dropna()\n",
    "model = ARIMA(endog=train_diff, order=(8, 0, 8)) \n",
    "model_fitted = model.fit()\n",
    "\n",
    "# Save the retrained model\n",
    "with open(\"retrained_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model_fitted, file)\n",
    "\n",
    "# Upload the retrained model to Azure Data Lake\n",
    "upload_file_to_blob(\"retrained_model.pkl\", container_name, \"model/model_pickel.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ece315d9-8bc8-4c7c-ab19-208e8affb282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_retraining",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
